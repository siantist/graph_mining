{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "020287e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "g = nx.Graph()\n",
    "\n",
    "g.add_nodes_from([0,1,2, 3,4,5])\n",
    "\n",
    "g.add_edges_from([(1, 2), (1, 3), (2,3), (3,1), (4,5), (1,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e747ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = nx.adjacency_matrix(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11c44401",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1= nx.to_numpy_matrix(g)\n",
    "# https://networkx.org/documentation/stable/reference/generated/networkx.convert_matrix.to_numpy_matrix.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56750c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 1., 1., 0.],\n",
       "        [0., 1., 0., 1., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9259f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm 1 (cycle bases suurvey)\n",
    "T = spanning_tree(g)\n",
    "\n",
    "cycles = []\n",
    "for i in range(v):\n",
    "    # non zero vector S_i \n",
    "    # cehck S_i(e) =0 for e in T\n",
    "    for e in T:\n",
    "        sie = Si(e)\n",
    "        if sie!=0: \n",
    "            print(\"Another vector Si\")\n",
    "# \n",
    "\n",
    "# Algorithm in 5.7.3 : replace line 4 of Algo 1 with find min wt isometric circuit C_i with <C_i, S_i> != 0\n",
    "\n",
    "# pg 52: circuit is isometric if for any u v on it, the p_{uv} best path conn. them is contained in it;\n",
    "# pg 49 to 51: compuute best paths\n",
    "\n",
    "def maintain_basis_orthog_space():\n",
    "    # initialize S_j \n",
    "    S= []\n",
    "    for j in range(v):\n",
    "        S_j = np.zeros(m)\n",
    "        \n",
    "        for i in range(m): # number of edges m\n",
    "            if i==j:\n",
    "                S_j[i] = 1# set to 1 if i=j\n",
    "        S[j] = S_j\n",
    "    for i in range(v):\n",
    "        # compuute min wt cycle C_i in H with <C_i, S_i> neq 0\n",
    "        \n",
    "\n",
    "# compute min wt cycle from H\n",
    "def min_wt_cycle(H, S):\n",
    "    # first check if <C_i , S_i> neq 0\n",
    "    min_wt_cycle_set= []\n",
    "    for h in H:\n",
    "        if np.dot(h, S) != 0:\n",
    "            # find weight\n",
    "            w_h = add_weights(h)\n",
    "\n",
    "def add_weights(h):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ecdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quotient graph , by triangles\n",
    "\n",
    "# https://networkx.org/documentation/stable/tutorial.html\n",
    "\n",
    "# https://www.kite.com/python/docs/networkx.quotient_graph\n",
    "\n",
    "all_cliques= nx.enumerate_all_cliques(g)\n",
    "\n",
    "triad_cliques=[x for x in all_cliques if len(x)==3 ]\n",
    "\n",
    "Q = nx.quotient_graph(g, is_triangle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "125e6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quotient graph, by cycles ? cliques?\n",
    "\n",
    "cycle_basis = nx.cycle_basis(g, 0) # 0 is root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "622f914d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 1]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cycle_basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40d3e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/57661314/fastest-way-to-find-all-cycles-of-length-n-containing-a-specified-node-in-a-dire\n",
    "\n",
    "def findPaths(G,u,n):\n",
    "    if n==0:\n",
    "        return [[u]]\n",
    "    paths = [[u]+path for neighbor in G.neighbors(u) for path in findPaths(G,neighbor,n-1)]\n",
    "    return paths\n",
    "\n",
    "def find_cycles(G,u,n):\n",
    "    paths = findPaths(G,u,n)\n",
    "    return [tuple(path) for path in paths if (path[-1] == u) and sum(x ==u for x in path) == 2]\n",
    "\n",
    "#cycles = find_cycles(DG,'A',4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccf984bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyc1 = find_cycles(g, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21467d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "285e2a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findPaths(g, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# touching number of cycles in the cycle basis\n",
    "\n",
    "# https://networkx.org/documentation/stable/_modules/networkx/algorithms/cycles.html\n",
    "\n",
    "touching_num = 0\n",
    "touching_edges = []\n",
    "\n",
    "for cycle in cycle_basis:\n",
    "    # look up edges with vertices from cycle\n",
    "    for v in cycle:\n",
    "        v_row = adj_mat[v]\n",
    "        output = [idx for idx, element in enumerate(v_row) if element ==1]\n",
    "        for edge_vertex in output:\n",
    "            if edge_vertex in cycle:\n",
    "                # do not add\n",
    "            else:\n",
    "                # add\n",
    "                touching_edges.append((v, edge_vertex))\n",
    "                touching_num = touching_num + 1\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d30fb63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "# https://www.kite.com/python/answers/how-to-find-the-index-of-list-elements-that-meet-a-condition-in-python\n",
    "\n",
    "a_list = [0, 1, 1, 0, 1, 0]\n",
    "\n",
    "output = [idx for idx, element in enumerate(a_list) if element ==1]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b96c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# touching nuumber (pg 26)\n",
    "\n",
    "max_n = 10 \n",
    "# t_n of n cycle : range of n \n",
    "for n in range(max_n):\n",
    "    # any cycles with length n\n",
    "    for u in range(nvertices):\n",
    "        cycs = find_cycles(G, u, n)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c514e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73fd94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6, 4]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "list1 = [1,2,3,4,5,6,7,8,9]\n",
    "random.choices(list1, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1050c282",
   "metadata": {},
   "source": [
    "https://courses.engr.illinois.edu/cs374/fa2020/lec_prerec/18/18_2_2_0.pdf \n",
    "\n",
    "hop based  recursion\n",
    "fix source s (assume all nodes can be reached by s in G)\n",
    "\n",
    "d(v, k) : shortest walk length from s to v using at most k edges ( dist(s,v) = d(v, n-1) ) \n",
    "\n",
    "recursion:\n",
    "d(v,k) = min $\\begin{cases}  min_{ u \\in V} ( d(u, k-1) + \\ell(u,v))  \\\\ d(v,k-1) \\end{cases}$ \n",
    "\n",
    "\n",
    "\n",
    "https://www.quora.com/Is-it-possible-to-use-the-powers-of-an-adjacency-matrix-to-compute-the-shortest-paths-BFS-would-compute \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdcc546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build hopset, https://www.cc.gatech.edu/~rpeng/CS7510_F19/Nov19Hopset.pdf\n",
    "\n",
    "# pick sqrt(n) random vertices -> S\n",
    "\n",
    "n=10\n",
    "sn = np.sqrt(n) \n",
    "sn_cap = np.ceil(sn)\n",
    "\n",
    "S = random.choices(vertices, k=sn_cap)\n",
    "\n",
    "# compute shortest path from those vertices to every other node up to sqrt(n) hops\n",
    "\n",
    "# hop-based Bellman Ford algorithm\n",
    "\n",
    "\n",
    "# Compute all pairs shortest path between vertices in S (using matrix powering)\n",
    "\n",
    "\n",
    "# https://resources.mpi-inf.mpg.de/departments/d1/teaching/ss12/AdvancedGraphAlgorithms/Slides14.pdf\n",
    "# fast matrix mult -> all pairs shortest path\n",
    "\n",
    "# n power of matrix gives distances by paths that use at most k edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8b0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run bellman ford uup to k steps \n",
    "import sys\n",
    "\n",
    "def bellmanford(vertices, edges, source, k):\n",
    "    distance = [] # list of size n\n",
    "    predecessor =[] # list of size n\n",
    "    \n",
    "    # initialize graph\n",
    "    for v in range(n):\n",
    "        distance[n] = sys.maxsize\n",
    "        predecessor[v] = None\n",
    "    \n",
    "    # relax edges repeatedly : |v| -1 times \n",
    "    for j in range(k):\n",
    "        for e in edges:\n",
    "            wt = edges[e]\n",
    "            if distance[u] != sys.maxsize and distance[u] + wt < distance[v]:\n",
    "                distance[v] = distance[u] + wt\n",
    "                predecessor[v] = u\n",
    "    # check for neg wt cycles\n",
    "    \n",
    "    return distance, predecessor\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6d0e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "adj_mat = np.array([[0,1,0,0],[1,0,1,0], [0,1,0,1], [0,0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91cee73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cd56d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0]\n",
      " [0 2 0 1]\n",
      " [1 0 2 0]\n",
      " [0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# n power of matrix gives distances by paths that use at most k edges\n",
    "\n",
    "matpow = adj_mat\n",
    "for i in range(k):\n",
    "    matpow = np.matmul(matpow, adj_mat)\n",
    "\n",
    "print(matpow)\n",
    "\n",
    "# check for entry in column  : k+1 length path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf155a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "val1 = sys.maxsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0009fedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val1==sys.maxsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40fe8b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Homomorphism_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb1a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# near-homomorphism or homo - other factor \n",
    "\n",
    "# check for homomorphism structure between graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12721894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform graph -> homomorphisms?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e609aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = [0, 4, 3, 0, 0, 1]\n",
    "res = [idx for idx, val in enumerate(row) if val != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e09a88",
   "metadata": {},
   "source": [
    "Step_k graph:\n",
    "it has same vertices as G, and distinct vertices are adjacent iff there is some walk of length k between them in G. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af975647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step_k graph of G \n",
    "\n",
    "def step_k(G, k):\n",
    "    #  take k-th power and check for entry in coluumn\n",
    "    matpow = adj_mat\n",
    "    for i in range(k):\n",
    "        matpow = np.matmul(matpow, adj_mat)\n",
    "    # return this matpow with nonzero entries equal 1 \n",
    "    \n",
    "    # edge list \n",
    "    edge_list = []\n",
    "    edge1= 0\n",
    "    for row in matpow:\n",
    "        # check entries equal 1\n",
    "        res = [idx for idx, val in enumerate(row) if val != 0]\n",
    "        for r in res:\n",
    "            edge_list.append((edge1, r))\n",
    "            edge_list.append((r, edge1))\n",
    "        edge1 = edge1+ 1\n",
    "    \n",
    "    return edge_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09caf8ad",
   "metadata": {},
   "source": [
    "Def. homomorphism density w.r.t. graph $H$: \n",
    "\n",
    "$t(H, G) = | hom(H,G)| / |V(G)|^{|V(H)|}\n",
    "\n",
    "where $hom(H,G)$ is the set of graph homomorphisms / adjacency preserving maps from H to G.\n",
    "\n",
    "triangle density: $\\int_0^1 \\int_0^1 \\int_0^1 w_g(u_1, u_2) w_g ( u_2, u_3) w_g(u_3, u_1) du_1 du_2 du_3$\n",
    "\n",
    "k-clique density:  ( k = 4)\n",
    "\n",
    "\n",
    "$\\int_0^1 \\int_0^1 \\int_0^1 \\int_0^1 w_g(u_1, u_2) w_g ( u_2, u_3) w_g(u_3, u_1) w_g(u_1,u_4) w_g(u_2, u4) w_g(u_3, u_4) du_1 du_2 du_3 du_4 $\n",
    "\n",
    "idea: count homomorphisms from hypergraph to non-hypergraph?\n",
    "\n",
    "A graph homomorphism is a mapping between two graphs that respects their structure: a function $\\phi: V(G) \\rightarrow V(H)$ from G to H if preserves edges; for edge [u,v] of G, $[\\phi(u), \\phi(v)]$ is an edge of H. \n",
    "\n",
    "https://mast.queensu.ca/~ctardif/articles/ghss.pdf \n",
    "\n",
    "Many results in extremal graph theory can be described by inequalities involving homomorphism densities associated to a graph. The following are a sequence of examples relating the density of triangles to the density of edges.\n",
    "\n",
    "Density can also be interpreted as the probability that a map from the vertices of H to the vertices of G chosen uniformly at random is a graph homomorphism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511990f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph physics - informed NN\n",
    "\n",
    "class HFMI(object): # hidden fluid mechanics ice\n",
    "\n",
    "    def __init__(self, x_data, y_data, u_data, v_data, s_data, h_data,b_data,x_eqns, y_eqns, layers, batch_size):\n",
    "    \n",
    "        self.layers = layers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # data\n",
    "        [self.x_data, self.y_data, self.u_data, self.v_data, self.s_data, self.h_data, self.b_data] = [x_data, y_data, u_data, v_data, s_data, h_data, b_data]\n",
    "        [self.x_eqns, self.y_eqns] = [x_eqns, y_eqns]\n",
    "\n",
    "\n",
    "        # change above to x, y, u,v,s,h ,b \n",
    "        [self.x_data_tf, self.y_data_tf, self.u_data_tf, self.v_data_tf, self.s_data_tf, self.h_data_tf, self.b_data_tf]  = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(7)]\n",
    "        [self.x_eqns_tf, self.y_eqns_tf] = [tf.placeholder(tf.float32, shape=[None, 1]) for _ in range(2)]\n",
    "\n",
    "        # physics uninformed NNs \n",
    "        self.net_uvshb = neural_net(self.x_data, self.y_data, layers = self.layers)\n",
    "\n",
    "        [self.u_data_pred,self.v_data_pred, self.s_data_pred, self.h_data_pred, self.b_data_pred] = self.net_uvshb(self.x_data_tf, self.y_data_tf)\n",
    "\n",
    "        # physics informed NNs\n",
    "        [self.u_eqns_pred, self.v_eqns_pred, self.s_eqns_pred, self.h_eqns_pred, self.b_eqns_pred] = self.net_uvshb(self.x_eqns_tf, self.y_eqns_tf)\n",
    "\n",
    "        [self.e1_eqns_pred,self.e2_eqns_pred, self.e3_eqns_pred] = IceSheet2D(self.u_eqns_pred,\n",
    "                                                      self.v_eqns_pred,\n",
    "                                                      self.s_eqns_pred,\n",
    "                                                      self.h_eqns_pred,\n",
    "                                                      self.b_eqns_pred,\n",
    "                                                      self.x_eqns_tf,\n",
    "                                                      self.y_eqns_tf)\n",
    "\n",
    "        self.loss = mean_squared_error(self.u_data_pred, self.u_data_tf) + mean_squared_error(self.v_data_pred, self.v_data_tf)+ mean_squared_error(self.s_data_pred, self.s_data_tf) + mean_squared_error(self.h_data_pred, self.h_data_tf)+ mean_squared_error(self.b_data_pred, self.b_data_tf) + mean_squared_error(self.e1_eqns_pred, 0.0) + mean_squared_error(self.e2_eqns_pred, 0.0) + mean_squared_error(self.e3_eqns_pred, 0.0) \n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "        self.sess = tf_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a1e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quotient graph , by triangles\n",
    "\n",
    "# https://networkx.org/documentation/stable/tutorial.html\n",
    "\n",
    "# https://www.kite.com/python/docs/networkx.quotient_graph\n",
    "\n",
    "all_cliques= nx.enumerate_all_cliques(g)\n",
    "\n",
    "triad_cliques=[x for x in all_cliques if len(x)==3 ]\n",
    "\n",
    "Q = nx.quotient_graph(g, is_triangle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836affc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quotient graph, by cycles ? cliques?\n",
    "\n",
    "cycle_basis = nx.find_cycle(g, orientation=\"ignore\")\n",
    "\n",
    "all_cliques= nx.enumerate_all_cliques(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c40de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilistic low diameter decomp's\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1309.3545.pdf: Improved Parallel Algos for Spanners and Hopsets\n",
    "\n",
    "# graph decomposition called Exponential Start Time Clustering (Algo. 1)\n",
    "\n",
    "def exp_start_time_cluster(g, beta, distances):\n",
    "    # clusters\n",
    "    clusters = {}\n",
    "    # for eac vertex\n",
    "    n_ver = len(g)\n",
    "    deltas = {}\n",
    "    dist_trees = {} # dictionary of dictionaries\n",
    "    for i in range(n_ver):\n",
    "        # draw exp(beta)\n",
    "        draw = np.random.exponential(scale=beta)\n",
    "        deltas[i] = draw\n",
    "    # calculate\n",
    "    for i in range(n_ver): # v in V \n",
    "        dist = {}\n",
    "        for j in range(n_ver): # u in V\n",
    "            d = distances[(i,j)]\n",
    "            arg = d - deltas[j]\n",
    "            dist[j] = arg\n",
    "        dist_trees[i] = dist\n",
    "    \n",
    "    for i in range(n_ver):\n",
    "        current_list = clusters.get(i)\n",
    "        # min dist\n",
    "        dist = dist_trees[i]\n",
    "        min_dist = min(dist)\n",
    "        min_ver = dist.index(min_dist)\n",
    "        if current_list==None:\n",
    "            clusters[i] = [min_ver]\n",
    "        else:\n",
    "            current_list.append(min_ver)\n",
    "            clusters[i] = current_list\n",
    " # return : clusters, spanning tree on each cluster rooted at its center\n",
    "    return clusters, dist_trees\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b837f0",
   "metadata": {},
   "source": [
    "$(\\epsilon, h, \\epsilon)$ hopset: a set of edges E' such that\n",
    "1. $|E'| \\leq m'$\n",
    "2. Each edge uv in E' corresponds to a uv-path in G suuch that $w(uv) = w(p)$\n",
    "3. For any vertices u and v, with prob. 0.5 $dist^h_{E \\cup E'} (u,v) \\leq (1+\\epsilon) dist_E(u,v)$.\n",
    "\n",
    "Klein and Subramian approximates length of path can be found efficiently.\n",
    "\n",
    "Our hop-set construction is based on recursive application of the exponential start time clustering\n",
    "from Section 2.1. We will designate some of the clusters produced, specifically the larger ones, as\n",
    "special. Since each vertex belongs to at most one cluster, there cannot be too many large clusters.\n",
    "As a result we can afford to compute distances from their centers to all other vertices, and keep a\n",
    "subset of them as hopset edges in the graph. There are two kinds of edges that we keep:\n",
    "1. star edges between the center of a large cluster and all vertices in that cluster.\n",
    "2. clique edges between the center of a large cluster and the centers of all other large clusters.\n",
    "\n",
    "In other words, in building the hopset we put a star on top of each large cluster and connect their\n",
    "centers into a clique. Then if our optimal s-t path $p^∗$\n",
    "encounters two or more of these large clusters,\n",
    "we can jump from the first to the last by going through their centers.\n",
    "\n",
    "This allows us to replace what hopefully is a large part of p\n",
    "∗ with only three edges: two star\n",
    "edges and one clique edge. However this replacement may increase the length of the path by the\n",
    "diameter of the large clusters. But as this distortion can only happen once, it is acceptable as long\n",
    "as the diameter of the clusters are less than \u000fw(p\n",
    "∗\n",
    "). Our algorithm then recursively builds hopsets\n",
    "on the small clusters. The probabilistic guarantees of an edge being cut gives that p\n",
    "∗ does not\n",
    "interact with too many such clusters.\n",
    "\n",
    "2 parameters : beta with which decomp. routine run, and threshold $\\rho$ by which cluster is too large\n",
    "\n",
    "steps:\n",
    "\n",
    "1. Compute a exponential start time clustering with parameter beta\n",
    "2. Identify clusters with more than $n/\\rho$ vertices as large clusters.\n",
    "3. Construct star and clique edges from the centers of each large cluster.\n",
    "4. Recurse on the small clusters.\n",
    "\n",
    "goal: hopsets speed up parallel BFS\n",
    "\n",
    "paralel : https://www.machinelearningplus.com/python/parallel-processing-python/#:~:text=In%20python%2C%20the%20multiprocessing%20module,in%20completely%20separate%20memory%20locations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "777def20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f59659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm 4\n",
    "\n",
    "# application: hopsets -> shortest paths approximations with O(m poly log n) work\n",
    "\n",
    "call= 0\n",
    "def hopset(vertices, edges, beta, call):\n",
    "    # compuute exp start time clust. with beta\n",
    "    n_final = 3\n",
    "    n = len(vertices)\n",
    "    if len(vertices) < n_final:\n",
    "        break\n",
    "    exp_clustering, dist_trees =exp_start_time_cluster(g,beta,dist)\n",
    "    if call==0:\n",
    "        #pool = mp.Pool(mp.cpu_count())\n",
    "        # for each cluster X\n",
    "        for x in exp_clustering:\n",
    "            newbeta = (k/eps)*(np.log(n))*beta\n",
    "            hopset(x, x_edges, newbeta, call )\n",
    "        #pool.close()\n",
    "    else:\n",
    "        # large and small cluusters sets\n",
    "        thres = n/rho\n",
    "        large_clusters = []\n",
    "        small_clusters=[]\n",
    "        for x in exp_clustering:\n",
    "            # key is center of cluster : key is x\n",
    "            xval = exp_clustering[x]\n",
    "            size_xval = len(xval)\n",
    "            if size_xval>=thres:\n",
    "                large_clusters.append(x) # xval\n",
    "                # add star edge btw center and all v in X with wt dist(v,c) -> matrix form ?\n",
    "                for v in xval:\n",
    "                    edges[v][x] = 1 # dist(v,c): min of dist_trees[v] and dist_trees[c] (?)\n",
    "                    edges[x][v] = 1\n",
    "                for v in xval:\n",
    "                    # add edge\n",
    "                \n",
    "            else:\n",
    "                small_clusters.append(x) # xval \n",
    "        # for all pairs of large clusters X_1, X_2 with centers c_1, c_2 add cliquue edge (c_1, c_2)\n",
    "        for x1 in large_clusters:\n",
    "            for x2 in large_clusters:\n",
    "                # add to edges with weight dist(c1, c2)\n",
    "                edges.append()\n",
    "        # for each X in small clusters, recursively call HopSet in parallel \n",
    "        for x in small_clusters:\n",
    "            out = hopset(vertices, edges, beta, call)\n",
    "        \n",
    "    # output: input graph G augmented with a set of weighted edge E*\n",
    "    return edges\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a3d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# euler tour\n",
    "# https://en.wikipedia.org/wiki/Euler_tour_technique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba79802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph operator -> parameters -> features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phi_{k,n} operator has k-simplices of G as vertices\n",
    "# 2 subgraphs are adjacent if they lie in common simplex with at most m vertices (pg. 136) \n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.Delaunay.html\n",
    "\n",
    "# k simplex is complex graph with k+1 vertices\n",
    "\n",
    "all_cliques= nx.enumerate_all_cliques(g)\n",
    "\n",
    "k = 3\n",
    "m = 2\n",
    "k_cliques=[x for x in all_cliques if len(x)== k+1 ]\n",
    "\n",
    "# create new graph with vertex set k_cliques\n",
    "\n",
    "# map from index to set\n",
    "map1 = {}\n",
    "map2 = {}\n",
    "ind1=0\n",
    "\n",
    "for x in k_cliques:\n",
    "    map1[ind1] = x \n",
    "    map2[x] = ind1\n",
    "    ind1= ind1+1\n",
    "\n",
    "adj_mat = np.zeros((ind1,ind1))\n",
    "# check adjacency up to m vertices\n",
    "\n",
    "for c1 in k_cliques:\n",
    "    for c2 in k_cliques:\n",
    "        index1 = map2[c1]\n",
    "        index2 = map2[c2]\n",
    "        # compare lists for same elements\n",
    "        if c1 == c2:\n",
    "            continue\n",
    "        counter1 = collections.Counter(c1)\n",
    "        counter2 = collections.Counter(c2)\n",
    "        set_list1 = set(tuple(d for d in sorted(counter1)))\n",
    "        set_list2 = set(tuple(d for d in sorted(counter2)))\n",
    "        intersect = set_list1.intersection(set_list2)\n",
    "        ilen = len(intersect)\n",
    "        if ilen> m:\n",
    "            # add edge\n",
    "            adj_mat[index1, index2] = 1\n",
    "            adj_mat[index2, index1] = 1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1b3ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg 120 , middle graph\n",
    "# intersection graph of all 1 or 2 simplices of G: edges and triangles\n",
    "# Mid(G) = L(G \\circ K_1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a557c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# https://machinelearningmastery.com/time-series-datasets-for-machine-learning/\n",
    "shampoo = pd.read_csv(\"shampoo.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91f55765",
   "metadata": {},
   "outputs": [],
   "source": [
    "shampoo_y = shampoo[\"Sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7913393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shampoo_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9682989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array, 36 x 2\n",
    "sham_x = range(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3513561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sham_mat = np.zeros((36, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9178fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sham_mat[:, 0] = sham_x\n",
    "sham_mat[:,1] = shampoo_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78bb39dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0., 266.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sham_mat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5d720b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of     time  sales\n",
       "0      0  266.0\n",
       "1      1  145.9\n",
       "2      2  183.1\n",
       "3      3  119.3\n",
       "4      4  180.3\n",
       "5      5  168.5\n",
       "6      6  231.8\n",
       "7      7  224.5\n",
       "8      8  192.8\n",
       "9      9  122.9\n",
       "10    10  336.5\n",
       "11    11  185.9\n",
       "12    12  194.3\n",
       "13    13  149.5\n",
       "14    14  210.1\n",
       "15    15  273.3\n",
       "16    16  191.4\n",
       "17    17  287.0\n",
       "18    18  226.0\n",
       "19    19  303.6\n",
       "20    20  289.9\n",
       "21    21  421.6\n",
       "22    22  264.5\n",
       "23    23  342.3\n",
       "24    24  339.7\n",
       "25    25  440.4\n",
       "26    26  315.9\n",
       "27    27  439.3\n",
       "28    28  401.3\n",
       "29    29  437.4\n",
       "30    30  575.5\n",
       "31    31  407.6\n",
       "32    32  682.0\n",
       "33    33  475.3\n",
       "34    34  581.3\n",
       "35    35  646.9>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sham = {'time' : sham_x, 'sales' : shampoo_y}\n",
    "shampoo_df = pd.DataFrame(data=sham)\n",
    "shampoo_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf86459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the time series turned into -> graph\n",
    "\n",
    "# for t_c, yc with t_a < t_c < t_b, \n",
    "# y_c < y_b + (y_a - y_b) [ (t_b - t_c) / (t_b - t_a )]\n",
    "\n",
    "for row in sham_mat:\n",
    "    t_row = row[0]\n",
    "    val_row = row[1]\n",
    "    \n",
    "    # \n",
    "\n",
    "edges = []\n",
    "# faster way to check\n",
    "\n",
    "total_len = len(sham_mat)\n",
    "# fix y_a first \n",
    "\n",
    "for row_a in sham_mat:\n",
    "    t_a = row_a[0]\n",
    "    val_a = row_a[1]\n",
    "    \n",
    "    # iterate over y_b\n",
    "    for ind_b in range(2, total_len):\n",
    "        row_b = sham_mat[ind_b]\n",
    "        \n",
    "        t_b = row_b[0]\n",
    "        val_b = row_b[1]\n",
    "        \n",
    "        bool_c = True\n",
    "        \n",
    "        for ind_c in range(1, ind_b):\n",
    "            row_c = sham_mat[ind_c]\n",
    "            \n",
    "            t_c = row_c[0]\n",
    "            val_c = row_c[1]\n",
    "            \n",
    "            if val_c < val_b and (val_a - val_b) > 0:\n",
    "                # add edge\n",
    "                edges.append((t_a, t_b))\n",
    "                edges.append((t_b, t_a))\n",
    "            else:\n",
    "                # check condition\n",
    "                if val_c < val_b + (val_a - val_b)*(t_b - t_c)/(t_b -t_a):\n",
    "                    edges.append((t_a, t_b))\n",
    "                    edges.append((t_b, t_a))\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add features together\n",
    "\n",
    "\n",
    "# features: max clique size\n",
    "# touching number\n",
    "# max degree\n",
    "# degree distn \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dacebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (1) max clique size \n",
    "\n",
    "all_cliques= nx.enumerate_all_cliques(g)\n",
    "\n",
    "clique_sizes = [len(x) for x in all_cliques ]\n",
    "\n",
    "max_size = max(clique_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc1b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) max degree\n",
    "\n",
    "\n",
    "# degree distn \n",
    "\n",
    "\n",
    "# sum up every column/row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d0d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=10)\n",
    "clf.fit(train_x_sir, train_y)\n",
    "pred_y = clf.predict(test_x_sir)\n",
    "acc = accuracy_score(test_y, pred_y)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "803e84a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5]\n",
    "\n",
    "convert_tuple(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf2a2b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8m/pt7_wvd11vbfl5hhydrtd5t00000gn/T/ipykernel_926/2998980072.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mlabels_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0medge_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mPhi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# edit to aggregate the labels\n",
    "# April 6\n",
    "\n",
    "# (1) aggregate node labels ( vertex histogram )\n",
    "\n",
    "# (2) aggregate edge labels \n",
    "\n",
    "# Phi_{k,n} operator has k-simplices of G as vertices\n",
    "# 2 subgraphs are adjacent if they lie in common simplex with at most n vertices (pg. 136) \n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.Delaunay.html\n",
    "\n",
    "# k simplex is complex graph with k+1 vertices\n",
    "\n",
    "def convert_tuple(list):\n",
    "    return tuple(i for i in list)\n",
    "\n",
    "\n",
    "# assume labels are given from nodes \n",
    "labels_map = {1: [1,2], 2: [3,4], 3: [1,3]}\n",
    "\n",
    "#edge_labels = {[1,2]:1, [3,4]:2}\n",
    "\n",
    "def Phi(g, k,n, labels_map): # add labels_map arg \n",
    "    \n",
    "    all_cliques= nx.enumerate_all_cliques(g)\n",
    "\n",
    "    k_cliques=[x for x in all_cliques if len(x)== k+1 ]\n",
    "    \n",
    "    # must generate simplex of m vertices\n",
    "    n_cliques = [x for x in all_cliques if len(x) <= n]\n",
    "\n",
    "    # create new graph with vertex set k_cliques\n",
    "\n",
    "    # map from index to set\n",
    "    map1 = {}\n",
    "    map2 = {} # backwards map\n",
    "    ind1=0\n",
    "    \n",
    "    new_labels = {}\n",
    "\n",
    "    for x in k_cliques:\n",
    "        #print(\"x is\", x)\n",
    "        xtup = convert_tuple(x)\n",
    "        map1[ind1] = x         \n",
    "        map2[xtup] = ind1\n",
    "        \n",
    "        # aggregation of labels \n",
    "        label_list = []\n",
    "        for vertex in x:\n",
    "            vlabel = labels_map[vertex]\n",
    "            label_list.append(vlabel) # extend(vlabel)\n",
    "        new_labels[ind1] = label_list\n",
    "        \n",
    "        ind1= ind1+1\n",
    "\n",
    "    adj_mat = np.zeros((ind1,ind1))\n",
    "    new_edge_list = []\n",
    "\n",
    "    # check adjacency up to m vertices\n",
    "    ind1 =0\n",
    "    ind2 = 0\n",
    "    for c1 in k_cliques:\n",
    "        for c2 in k_cliques:\n",
    "            c1tup = convert_tuple(c1)\n",
    "            c2tup = convert_tuple(c2)\n",
    "            index1 = map2[c1tup]\n",
    "            index2 = map2[c2tup]\n",
    "            # compare lists for same elements\n",
    "            if ind1== ind2:#c1 == c2:\n",
    "                continue\n",
    "            counter1 = collections.Counter(c1)\n",
    "            counter2 = collections.Counter(c2)\n",
    "            set_list1 = set(tuple(d for d in sorted(counter1)))\n",
    "            set_list2 = set(tuple(d for d in sorted(counter2)))\n",
    "            intersect = set_list1.intersection(set_list2)\n",
    "            ilen = len(intersect)\n",
    "            print(\"ilen is\", ilen)\n",
    "            # modification to the input of the operator: if there is n=1 or more vertices in common\n",
    "            if ilen>= n:\n",
    "                # add edge\n",
    "                adj_mat[index1, index2] = 1\n",
    "                adj_mat[index2, index1] = 1\n",
    "                new_edge_list.append((index1, index2))\n",
    "                new_edge_list.append((index2, index1))\n",
    "                \n",
    "                # check & aggregate edge labels ?\n",
    "                \n",
    "            ind2 = ind2+1\n",
    "        ind1 = ind1+1\n",
    "\n",
    "    return adj_mat, new_edge_list, k_cliques # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a413ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grakel.kernels import Kernel\n",
    "\n",
    "from warnings import warn\n",
    "\n",
    "from collections import Counter\n",
    "from collections import Iterable\n",
    "\n",
    "from grakel.graph import Graph\n",
    "\n",
    "from numpy import zeros\n",
    "from numpy import einsum\n",
    "from numpy import array\n",
    "from numpy import squeeze\n",
    "\n",
    "from six import iteritems\n",
    "from six import itervalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8036e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd2ad2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertex histogram <-> phi_{k,n}\n",
    "\n",
    "class vertex_phi(Kernel):\n",
    "    \n",
    "    def __init__(self, k=3, n= 10, n_jobs=None):\n",
    "        super(vertex_phi, self).__init__(n_jobs=n_jobs)\n",
    "        self.k = k\n",
    "        self.n = n\n",
    "    \n",
    "    def _initialized(self):\n",
    "        if not self._initialized[\"n_jobs\"]:\n",
    "            if self.n_jobs is not None:\n",
    "                warn(\"hi\")\n",
    "            self._initialized[\"n_jobs\"] = True\n",
    "    \n",
    "    def parse_input(self,X):\n",
    "        # input is iterable with 1-3 features: adj mat / edge dictionary, node labels, edge labels\n",
    "        # check input is iterable : if not isinstance(X, Iterable): raise TypeError(\"input must be iterable\")\n",
    "        \n",
    "        rows, cols, data = list(), list(), list()\n",
    "        \n",
    "        self._method_calling = 1\n",
    "        print(\"self method calling:\", self._method_calling)\n",
    "        \n",
    "        if self._method_calling in [1,2]:\n",
    "            labels = dict()\n",
    "            self._labels = labels\n",
    "        elif self._method_calling==3:\n",
    "            labels= dict(self._labels)\n",
    "        \n",
    "        ni =0\n",
    "        for (i, x) in enumerate(iter(X)):\n",
    "            is_iter = isinstance(x, Iterable)\n",
    "            if is_iter:\n",
    "                x = list(x)\n",
    "            if is_iter and len(x) in [0,2,3]:\n",
    "                L = x[1]\n",
    "            elif type(x) is Graph:\n",
    "                L = x.get_labels(purpose=\"any\")\n",
    "            # construct data input for np array\n",
    "            for (label, frequency) in iteritems(Counter(itervalues(L))):\n",
    "                rows.append(ni)\n",
    "                \n",
    "                col_idx = labels.get(label, None)\n",
    "                \n",
    "                if col_idx is None:\n",
    "                    col_idx = len(labels)\n",
    "                    labels[label] = col_idx\n",
    "                # designate column info\n",
    "                cols.append(col_idx)\n",
    "                \n",
    "                # designate frequency value\n",
    "                data.append(frequency)\n",
    "            ni += 1\n",
    "            \n",
    "            \n",
    "        # init the feature matrix\n",
    "        \n",
    "        features = zeros(shape=(len(X), len(labels)))\n",
    "        features[rows, cols] = data\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _calculate_kernel_matrix(self, Y=None):\n",
    "        # calc kernel mat given target graph and kernel \n",
    "        if Y is None:\n",
    "            K = self.X.dot(self.X.T)\n",
    "        else:\n",
    "            K = Y[:, :self.X.shape[1]].dot(self.X.T)\n",
    "        \n",
    "        return K \n",
    "    \n",
    "    def diagonal(self):\n",
    "        # calculate kernel matrix diagonal of fitted data\n",
    "        self._X_diag = einsum('ij,ij->i', self.X, self.X)\n",
    "        Y_diag = einsum('ij, ij->i', self._Y, self._Y)\n",
    "        return self._X_diag, Y_diag\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c7813ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = vertex_phi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "917a0222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test vertex phi (histogram) kernel\n",
    "\n",
    "H2o_adjacency = [[0,1,1],[1,0,0],[1,0,0]]\n",
    "H2o_node_labels = {0: 'O', 1:'H', 2: 'H'}\n",
    "\n",
    "H2O_graph = Graph(initialization_object = H2o_adjacency, node_labels=H2o_node_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4426613d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self method calling: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [1., 2.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vp.parse_input((H2O_graph, H2O_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e082b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grakel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf2decc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grakel.kernels import VertexHistogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2edbd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "vh = VertexHistogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e07ce019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 5.],\n",
       "       [5., 5.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vh.fit_transform((H2O_graph, H2O_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d224e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = {1:2, 4:1, 5:1}\n",
    "lab2 = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9d9be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2, 4: 1, 5: 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53af2ea8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot convert dictionary update sequence element #0 to a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8m/pt7_wvd11vbfl5hhydrtd5t00000gn/T/ipykernel_926/1959654249.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlab2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert dictionary update sequence element #0 to a sequence"
     ]
    }
   ],
   "source": [
    "dict(lab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate graphs ( WL_labels_inverse, nl)\n",
    "\n",
    "# calc kernel matrix for 0 iteration \n",
    "\n",
    "\n",
    "# recalculate the labels\n",
    "new_graphs = list()\n",
    "\n",
    "\n",
    "# graph operator gives a way to aggregate vertex labels and edge labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba7ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle basis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to combine edge + vertex kernels (w.r.t local Rademacher complexity ?)\n",
    "\n",
    "# combine in subgraph matching ? https://github.com/ysig/GraKeL/blob/master/grakel/kernels/subgraph_matching.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2b45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414ff0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def centrality(v, sp_mat):\n",
    "    row = sp_mat[v]\n",
    "    mn = np.mean(row)\n",
    "    recip = 1/(mn)\n",
    "    return recip\n",
    "\n",
    "class ShortestPath(Kernel):\n",
    "\n",
    "    _graph_bins = dict()\n",
    "\n",
    "    def __init__(self, n_jobs=None,\n",
    "                 normalize=False,\n",
    "                 verbose=False,\n",
    "                 with_labels=True,\n",
    "                 algorithm_type=\"auto\"):\n",
    "        \"\"\"Initialize a `shortest_path` kernel.\"\"\"\n",
    "        super(ShortestPath, self).__init__(\n",
    "            n_jobs=n_jobs, normalize=normalize, verbose=verbose)\n",
    "\n",
    "        self.with_labels = with_labels\n",
    "        self.algorithm_type = algorithm_type\n",
    "        self._initialized.update({\"with_labels\": False, \"algorithm_type\": False})\n",
    "        \n",
    "        \n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize all transformer arguments, needing initialization.\"\"\"\n",
    "        if not self._initialized[\"n_jobs\"]:\n",
    "            if self.n_jobs is not None:\n",
    "                warnings.warn('no implemented parallelization for ShortestPath')\n",
    "            self._initialized[\"n_jobs\"] = True\n",
    "\n",
    "        if not self._initialized[\"algorithm_type\"]:\n",
    "            if self.algorithm_type == \"auto\":\n",
    "                self._graph_format = \"auto\"\n",
    "            elif self.algorithm_type == \"floyd_warshall\":\n",
    "                self._graph_format = \"adjacency\"\n",
    "            elif self.algorithm_type == \"dijkstra\":\n",
    "                self._graph_format = \"dictionary\"\n",
    "            else:\n",
    "                raise ValueError('Unsupported \"algorithm_type\"')\n",
    "\n",
    "        if not self._initialized[\"with_labels\"]:\n",
    "            if self.with_labels:\n",
    "                self._lt = \"vertex\"\n",
    "                self._lhash = lhash_labels\n",
    "                self._decompose_input = decompose_input_labels\n",
    "            else:\n",
    "                self._lt = \"none\"\n",
    "                self._lhash = lhash\n",
    "                self._decompose_input = decompose_input\n",
    "    \n",
    "    def parse_input(self, X):\n",
    "        # returns sp_counts: dictionary for each vertex holds the counts of shortest path tuples\n",
    "        \n",
    "        # add: centrality : centrality scores of each vertex \n",
    "        i=0\n",
    "        for (idx, x) in enumerate(iter(X)):\n",
    "            if type(x) is Graph:\n",
    "                spm_data = x.build_shortest_path_matrix(self.algorithm_type, labels=self._lt)\n",
    "                \n",
    "                centralities = {}\n",
    "                # calculate centralities \n",
    "                for vertex in range(nvert):\n",
    "                    cent = centrality(vertex, spm_data)\n",
    "                    centralities[vertex] = cent\n",
    "                \n",
    "        S, L = self._decompose_input(spm_data)\n",
    "                sp_counts[i] = dict()\n",
    "                for u in range(S.shape[0]):\n",
    "                    for v in range(S.shape[1]):\n",
    "                        if u == v or S[u, v] == float(\"Inf\"):\n",
    "                            continue\n",
    "                        label = self._lhash(S, u, v, *L)\n",
    "                        if label not in self._enum:\n",
    "                            if self._method_calling == 1:\n",
    "                                idx = len(self._enum)\n",
    "                                self._enum[label] = idx\n",
    "                            elif self._method_calling == 3:\n",
    "                                if label not in self._Y_enum:\n",
    "                                    idx = len(self._enum) + len(self._Y_enum)\n",
    "                                    self._Y_enum[label] = idx\n",
    "                                else:\n",
    "                                    idx = self._Y_enum[label]\n",
    "                        else:\n",
    "                            idx = self._enum[label]\n",
    "                        if idx in sp_counts[i]:\n",
    "                            sp_counts[i][idx] += 1\n",
    "                        else:\n",
    "                            sp_counts[i][idx] = 1\n",
    "            \n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit and transform, on the same dataset.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : iterable\n",
    "            Each element must be an iterable with at most three features and at\n",
    "            least one. The first that is obligatory is a valid graph structure\n",
    "            (adjacency matrix or edge_dictionary) while the second is\n",
    "            node_labels and the third edge_labels (that fitting the given graph\n",
    "            format).\n",
    "        y : Object, default=None\n",
    "            Ignored argument, added for the pipeline.\n",
    "        Returns\n",
    "        -------\n",
    "        K : numpy array, shape = [n_targets, n_input_graphs]\n",
    "            corresponding to the kernel matrix, a calculation between\n",
    "            all pairs of graphs between target an features\n",
    "        \"\"\"\n",
    "        self._method_calling = 2\n",
    "        self.fit(X)\n",
    "\n",
    "        # calculate feature matrices.\n",
    "        phi_x = np.zeros(shape=(self._nx, len(self._enum)))\n",
    "\n",
    "        for i in self.X.keys():\n",
    "            for j in self.X[i].keys():\n",
    "                phi_x[i, j] = self.X[i][j]\n",
    "\n",
    "        # Transform - calculate kernel matrix\n",
    "        self._phi_X = phi_x\n",
    "        km = np.dot(phi_x, phi_x.T)\n",
    "\n",
    "        self._X_diag = np.diagonal(km)\n",
    "        if self.normalize:\n",
    "            return np.divide(km, np.sqrt(np.outer(self._X_diag, self._X_diag)))\n",
    "        else:\n",
    "            return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchy defined by the random walk (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct product graph of transformed graphs (?)\n",
    "\n",
    "#https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.operators.product.cartesian_product.html\n",
    "\n",
    "G = nx.Graph()\n",
    "H = nx.Graph()\n",
    "\n",
    "G.add_node(0,a1=True)\n",
    "H.add_node('a',a2='Spam')\n",
    "P = nx.cartesian_product(G,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151764ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare G1 with G1 x G2 (dir product), G2 with G1 x G2 , add together the matrices in boolean way ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal assignment of different boolean products/sums (?)\n",
    "\n",
    "# based on different groupings of vertices (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5741a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bits=8\n",
    "_max_number = 10\n",
    "_mask = _max_number-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d56ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bit operation (try it out)\n",
    "def ROT( n, d):\n",
    "        \"\"\"`rot` operation for binary numbers.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            The value which will be rotated.\n",
    "        d : int\n",
    "            The number of rotations.\n",
    "        Returns\n",
    "        -------\n",
    "        rot : int\n",
    "            The result of a rot operation.\n",
    "        \"\"\"\n",
    "        m = d % bits\n",
    "\n",
    "        if m > 0:\n",
    "            return (n << m) & _mask | \\\n",
    "                   ((n & _mask) >> (bits-m))\n",
    "        else:\n",
    "            return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0ba169c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROT(5 ,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac28e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a7b1531",
   "metadata": {},
   "source": [
    "take direct product -> \n",
    "boolean multiplication (between original and dir product) \n",
    "-> optimal score ?\n",
    "\n",
    "\n",
    "graph transform / grouping of vertices and edges -> subgraph kernel (combine edge and vertex) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct product of graphs\n",
    "\n",
    "def direct_prod(g1, g2, v_labels1, v_labels2, e_labels1, e_labels2):\n",
    "    \n",
    "    n = len(g1)\n",
    "    \n",
    "    # first generate vertices (v1,v2) \n",
    "    vertices1x2 = {}\n",
    "    inverse_vertices1x2= {}\n",
    "    ind=0\n",
    "    \n",
    "    v1 = 0\n",
    "    for lab1 in v_labels1:\n",
    "        v2 = 0\n",
    "        for lab2 in v_labels2:\n",
    "            \n",
    "            if lab1 == lab2:\n",
    "                \n",
    "                pair = (v1, v2)\n",
    "\n",
    "                vertices1x2[ind] = pair\n",
    "                inverse_vertices1x2[pair] = ind\n",
    "            ind= ind+1\n",
    "            v2 +=1\n",
    "        v1+=1\n",
    "            \n",
    "    \n",
    "    # iterate over adj matrices and edge lists \n",
    "    \n",
    "    e_labels3 = np.zeros((n,n))\n",
    "    \n",
    "    g1_row = 0\n",
    "    g2_row = 0\n",
    "    for row1 in g1:\n",
    "        for row2 in g2: \n",
    "            g1_col = 0\n",
    "            g2_col = 0\n",
    "            for e1 in row1:\n",
    "                \n",
    "                for e2 in row2:\n",
    "                    \n",
    "                    # check condition (u_1, u_2) in E1 and (v_1, v_2) in E2 and labele(u_1,u_2) = labele(v_1, v_2)\n",
    "                    if e1 != 0:\n",
    "                        if e2!=0:\n",
    "                            # check label\n",
    "                            label1 = e_labels1[g1_row, g1_col]\n",
    "                            label2 = e_labels2[g2_row, g2_col]\n",
    "                            \n",
    "                            if label1==label2:\n",
    "                                # add edge\n",
    "                                v1 = inverse_vertices1x2[(g1_row, g1_col)]\n",
    "                                v2 = inverse_vertices1x2[(g2_row, g2_col)]\n",
    "                                e_labels3[v1, v2] = 1\n",
    "                    \n",
    "                    g2_col +=1\n",
    "                g1_col+=1\n",
    "            g2_row +=1\n",
    "        g1_row +=1\n",
    "                        \n",
    "                    \n",
    "# for efficiency, direct product of grouped vertices;\n",
    "\n",
    "\n",
    "\n",
    "# group together vertices / edges based on closeness\n",
    "\n",
    "# group together vertices / edges based on min wt cycle basis\n",
    "\n",
    "\n",
    "# optimal assignment search based on (?) topological similarity\n",
    "\n",
    "\n",
    "# independent sets based on being farther away from each other; start comparison between different groups\n",
    "\n",
    "## compute the expander-ness of each group / set to compare "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1084595e",
   "metadata": {},
   "source": [
    "topological similarity measure for proteins\n",
    "\n",
    "topological measures of similarity\n",
    "\n",
    "- bar visibility of a graph G: representation where each vertex is mapped to bar; any 2 vertices are connected in G iff corresponding bars have vertical line segment connecting them and intersects no other bar \n",
    "\n",
    "-> use also for visibility graphs\n",
    "\n",
    "-- simple homotopy height\n",
    "\n",
    "- searching (parameter) : how long to sweep a graph : node searching connection to homotopy height\n",
    "\n",
    "\n",
    "--> tree-like ness? (how many cycles in basis divided by total # of leaves ?) \n",
    "\n",
    "https://z2pack.greschd.ch/en/latest/_downloads/75eec5bb13ab1589c8b65a3cc3781afd/z2pack_chapter.pdf\n",
    "\n",
    "Calculating topological invariants with Z2Pack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ad1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_row=0\n",
    "    g2_row =0\n",
    "    for row1 in g1:\n",
    "        for row2 in g2:\n",
    "            g1_col=0\n",
    "            g2_col=0\n",
    "            for e1 in row1:\n",
    "                for e2 in row2:\n",
    "                    lab1 = v_labels1[g1_row, g1_col]\n",
    "                    lab2 = v_labels2[g2_row, g2_col]\n",
    "                    if lab1==lab2:\n",
    "                        pair = \n",
    "                    g2_col+=1\n",
    "                g1_col+=1\n",
    "            \n",
    "            g2_row+=1\n",
    "        g1_row+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fc9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kronecker prod yields homomorphism; https://en.wikipedia.org/wiki/Tensor_product_of_graphs\n",
    "\n",
    "# what does boolean product yield?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217822ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# breaking up the graph into different subgraphs / groups\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99cc767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A study of visibility graphs for time series representations\n",
    "\n",
    "# properties: diameter d: max len of all shortest paths found\n",
    "# btwn any pairs of nodes in (subgraph G' or graph G)\n",
    "\n",
    "def diameter(g_shortest_paths_mat):\n",
    "    return max(g_shortest_paths_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c074dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# properties: clustering coefficient (a study of visibility graphs for )\n",
    "\n",
    "# for any node i, ratio of # of edges btwn its nbrs and # of possible edges\n",
    "\n",
    "def clustering_coefficient(i, g): # node i in graph g (adj mat)\n",
    "    # calculate number edges for each vertex\n",
    "    n_edges=[]\n",
    "    for row in g:\n",
    "        n_edges.append(sum(row))\n",
    "    \n",
    "    # k_i nbrs \n",
    "    row_i = g[i]\n",
    "    k_i = sum(row_i)\n",
    "    # E_i : number of edges btwn i's nbrs\n",
    "    E_i = 0\n",
    "    ind = 0\n",
    "    for el in row_i:\n",
    "        if el== 1:\n",
    "            n_current = n_edges[ind]\n",
    "            E_i = E_i + n_current\n",
    "        ind = ind+1\n",
    "    C_i = 2*E_i/(k_i*(k_i-1))\n",
    "    return C_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3be5d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps = {}\n",
    "gps[0] = [1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32ff70ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for el in gps:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc530ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition them by searching for leaves (degree 1 ) and vertices of degree 2 , slowly adding their neighbors to the group \n",
    "\n",
    "deg1v = []\n",
    "deg2v = []\n",
    "\n",
    "groups1 = {}\n",
    "groups2 = {}\n",
    "ind1 = 0\n",
    "ind2 = 0\n",
    "\n",
    "ind=0\n",
    "for row in adj_mat:\n",
    "\tdeg_row = sum(row)\n",
    "\tif deg_row==1:\n",
    "\t\tdeg1v.append(ind)\n",
    "        groups1[ind1] = ind\n",
    "\tif deg_row==2:\n",
    "\t\tdeg2v.append(ind)\n",
    "        groups2[ind2] = ind\n",
    "    \n",
    "\tind=ind+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480328a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal assignment of groups based on these roots ; the groups may overlap : if they overlap, then use the \n",
    "# boolean product ? \n",
    "\n",
    "# propagate for k steps \n",
    "# from leaves \n",
    "n_groups = ind \n",
    "k = 5\n",
    "\n",
    "for g in groups1:\n",
    "    root = groups1[g]\n",
    "    current_gp = [root]\n",
    "    # extract the neighbors \n",
    "    row = np.array(adj_mat[root])\n",
    "    nbrs = np.where(row == 1)\n",
    "    current_gp.extend(nbrs)\n",
    "    groups1[g] = current_gp \n",
    "\n",
    "# continue\n",
    "\n",
    "for steps in range(k-1):\n",
    "    # join the neighbors to the groups\n",
    "    for g in groups1:\n",
    "        roots = groups1[g] \n",
    "        current_gp = roots\n",
    "        for r in roots:\n",
    "            row = np.array(adj_mat[r])\n",
    "            nbrs = np.where(row == 1)\n",
    "            current_gp.extend(nbrs)\n",
    "        groups1[g] = np.unique(current_gp)\n",
    "    \n",
    "# count the number of matching vertex or edge labels between the groups of the graphs\n",
    "\n",
    "vertex_labels = [] # input \n",
    "\n",
    "vlab_map = {}\n",
    "\n",
    "for g in groups1:\n",
    "    v_labels = []\n",
    "    for vertex in g:\n",
    "        lab = vertex_labels[vertex]\n",
    "        v_labels.append(lab)\n",
    "    \n",
    "    vlab_map[g] = v_labels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb016b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f52aab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 5]),)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "w = np.array([1,2,3,4,5,4])\n",
    "np.where(w == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fc568ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16069fcc",
   "metadata": {},
   "source": [
    "https://ysig.github.io/GraKeL/0.1a8/kernels/random_walk.html\n",
    "\n",
    "performing random walk on directed graph is equivalent to performing a simultaneous random walk on G1 and G2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58555426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max entropy random walk\n",
    "\n",
    "# trans prob defined in terms of components of eig centrality of node i \n",
    "# e_i is i-th component of normalized eigenvector corresponding to max eigenval of A \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
